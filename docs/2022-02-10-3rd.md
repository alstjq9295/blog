---
layout: default
title: "ElasticSearch 한국어 형태소 분석기 Nori Analyzer 에 관하여"
description: "MBTI 관련 인스타그램 데이터 기반 서비스 만들기"
categories:
    - Blog
tags:
    - Blog
date: 2022-02-10
last_modified_at: 2022-02-10
---
```
* 2022 1Q DMK Study
    - MBTI 관련 인스타그램 데이터 기반 서비스 만들기
```
# ElasticSearch 한국어 형태소 분석기 Nori Analyzer 에 관하여
* 참조 링크: 
    - [https://coding-start.tistory.com/167](https://coding-start.tistory.com/167)
    - [https://gritmind.blog/2020/07/22/nori_deep_dive/](https://gritmind.blog/2020/07/22/nori_deep_dive/)
    - [http://kimjmin.net/2019/08/2019-08-how-to-analyze-korean/](http://kimjmin.net/2019/08/2019-08-how-to-analyze-korean/)

## Nori analyzer
- 한국어 형태소 분석기
- Macab 기반의 일본어 형태소 분석기인 Kromoji 를 재활용
- Decompound Mode (복합명사 분석 모드):
    - None: 복합명사로 분리하지 않음
    - Discard: 복합명사로 분리 후 원본 데이터 삭제
    - Mixed: 복합명사로 분리 후 원본 데이테 유지
- Customization:
    - user_dictionary
    - nori_posfilter (stoptags 지정 품사 제거)
    - synonym_filtering (동의어 필터): 쉼표로 연결 (`A⇒B` 로 A 토큰을 B 로 치환하여 A는 토큰에 없음)
        - 참조: [https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html)
    - stop_filtering (불용서 필터): 등록된 불용어는 토큰에서 제외
        - 참조: [https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html)

### user_dictionary
- USER → KNOWN → UNKNOWN 순서로 토큰을 추출한다.
- nori_tokenizer 는 특수기호를 기준으로 형태소를 분리하기 때문에 "-"(대시) 같은 기호가 들어간 의약품 등의 키워드는 검색할 수 없다.  ⇒ **검색할 수 있다?! 문제는 기호 앞 뒤의**
    - nori_userdictionary 를 사용하여 해당 키워드 등록
    - 멀티필드를 사용하여 다중 analyzer 로 해당 필드를 구축하는 방법이 있다. nori_analyzer 와 whitespace analyzer 에 char_filter 를 추가하여 구성하는 방식이다.
    - 예시:
    ```javascript
    PUT test_idx_ms_00
    {
      "settings": {
        "analysis": {
          "analyzer": {
            "nori_analyzer": {
              "tokenizer": "nori_tokenizer",
              "decompound_mode": "mixed",
              "filter": ["lowercase"]
            }
          }
        }
      },
      "mappings": {
        "properties": {
          "content": {
            "type": "text",
            "fields": {
              "nori": {
                "type": "text",
                "analyzer": "nori_analyzer"
              },
              "whitespace": {
                "type": "text",
                "analyzer": "whitespace"
              }
            }
          }
        }
      }
    }
    ```

### Tokenizer: Standard VS Whitespace VS Letter
- Standard: 공백과 마침표, 느낌표 등의 기호를 제거하며 토큰화하는데 글자 사이에 연결된 언더스코어나 소수점을 표기하는 dot(예: Python3.6 의 마침표) 은 토큰화 기준이 되지도 제거되지도 않는다.
- Whitespace: 모든 공백(\s, \t, \r, \n 등)을 기준으로 토큰화하며 Standard 와는 다르게 기호를 제거하지도 않는다.
- Letter: 모든 공백과 숫자, 기호들을 기준으로 제거하며 토큰화
- 참조: [https://esbook.kimjmin.net/06-text-analysis/6.5-tokenizer/6.5.1-standard-letter-whitespace](https://esbook.kimjmin.net/06-text-analysis/6.5-tokenizer/6.5.1-standard-letter-whitespace)

### 마대리 1.0 에서 발생한 'user_dictionary' 의 문제점 (이거 적어도 되겠지..?)
- '데이터마케팅코리아'를 `'데이터'+'마케팅'+'하'+'고'+'하고'+'코'+'리아'` 로 토큰화
- nori_analyzer 는 기본적으로 소문자로 변환하기 때문에 sov Index 에서는 'lenovo' 와 'LENOVO' 둘 모두 잘 쿼리해오지만 user_dictionary 를  사용한 buzz Index 에서는 'LENOVO' 를 찾아오지 못함
    - ~~여기서 한가지 문제는 term 을 사용하면 sov 에서도 찾아오지 못하고 있다는 것인데 그 이유는 match 는 입력받은 조건을 소문자로 변환하여 Tokenizing 하여 쿼리하지만 term 은 입력받은 조건을 그대로 쿼리하기 때문에 쿼리를 바꿔야 할지도 모르겠다.~~
    - term / match_phrase / match 비교 (참조: [https://findstar.pe.kr/2018/01/19/understanding-query-on-elasticsearch/](https://findstar.pe.kr/2018/01/19/understanding-query-on-elasticsearch/))
        - term: 역색인에 저장된 token 중에 완벽히 일치하는 값이 있어야 반환한다.
        - match: 마찬가지로 역색인에 저장된 token 중 일치하는 값을 찾지만 해당 필드의analyzer 를 거친 결과의 token 으로 검색한다.
        - match_phrase: match 처럼 analyzer 를 통한 검색이나 두 개 이상의 토큰이 순서까지 동일해야 검색된다.
    - lenovo 와 LENOVO 에 대한 match 와 match_phrase 의 결과값은 동일하다. 소문자로 토크나이징 및 검색하기 위해서는 lowercase Token Filter 를 사용해야하는 것으로 보인다.
- userdic_ko.txt 파일을 수정하여 재적용하더라도 이미 적재된 데이터에 대해서는 적용되지 않는다.
